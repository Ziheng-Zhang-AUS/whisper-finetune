model:
  init_name: large-v2
  bfloat16: False # If this is set, mixed_precision_training will use bfloat16
  lora: False
  quantize_model: False # Experimental.
  lora_config:
    rank: 32
    lora_alpha: 64
    lora_dropout_p: 0.05
dataset:
  train_datasets: [i4ds/sds-stt-spc-train-val-v3]
  select_n_per_t_ds: [null]
  val_datasets: [i4ds/schoenleber_1]
  select_n_per_v_ds: [None]
  train_split_name: "train"
  valid_split_name: "test"
  no_timestamp_training: False
  max_prompt_length: 223
  prompt_use_rate: 0.5
  no_timestamp_rate: 0.5
  batch_size: 16
  batch_size_eval: 16
lr_scheduler:
  type: linear
  warmup_steps_ratio: 0.001953125 # (2048 / 1048576)
optimizer:
  type: adamw
  8bit: True
  params:
    lr: 2.0e-4
    weight_decay: 0.1
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False
training:
  accum_grad_steps: 16
  train_only_decoder: False
  train_only_encoder: False
  max_grad_norm: 1.0
  stochastic_depth: 0.1
  epochs: 3
  eval_steps: 0.25 # % of each epoch to do validation.
  save_all_checkpoints: False
  mixed_precision_training: True
  mp_dtype: fp16
  gradient_checkpointing_encoder: True
  gradient_checkpointing_encoder_last_only: False # Somehow, does not save a lot of memory (~1%).
  gradient_checkpointing_decoder: True
augmentation:
  spec_augment:
    apply: True
    time_mask_param: 100
    p: 1.0
    freq_mask_param: 27
    time_warp_w: 80
  audio_augment:
    apply: False
    lpf: # Low pass filter
      p: 0.1
      sample_rate: 16000
      target_rate: 16000
      min_cutoff_freq: 3340
      max_cutoff_freq: 7500
    hpf: # High pass filter
      p: 0.1
      sample_rate: 16000
      target_rate: 16000
      min_cutoff_freq: 20
      max_cutoff_freq: 128
    acn:  # Colored noise
      p: 0.0
      sample_rate: 16000
      target_rate: 16000
      min_snr_in_db: 25
      max_snr_in_db: 99
seed: 123
save_dir: output