{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch.nn as nn\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model('tiny', device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 384)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.model import MultiHeadAttention\n",
    "import loralib as lora\n",
    "import torch\n",
    "\n",
    "def replace_attention_layers_with_lora(module, config, parent=None, parent_name=None):\n",
    "    for name, child in module.named_children():\n",
    "        # Recursive call for child modules\n",
    "        replace_attention_layers_with_lora(child, config, parent=module, parent_name=name)\n",
    "\n",
    "    if isinstance(module, MultiHeadAttention):\n",
    "        # Replace the specific layers if they match the target names\n",
    "        if hasattr(module, 'query') and 'query' in config['target_modules']:\n",
    "            setattr(module, 'query', lora.Linear(module.query.in_features, module.query.out_features, r=config['r']))\n",
    "        if hasattr(module, 'key') and 'key' in config['target_modules']:\n",
    "            setattr(module, 'key', lora.Linear(module.key.in_features, module.key.out_features, r=config['r'], bias=False))\n",
    "        if hasattr(module, 'value') and 'value' in config['target_modules']:\n",
    "            setattr(module, 'value', lora.Linear(module.value.in_features, module.value.out_features, r=config['r']))\n",
    "\n",
    "def mark_only_lora_as_trainable(model, bias='none'):\n",
    "    lora.mark_only_lora_as_trainable(model, bias=bias)\n",
    "\n",
    "def save_lora_model(model, path, bias='none'):\n",
    "    torch.save(lora.lora_state_dict(model, bias=bias), path)\n",
    "\n",
    "def load_lora_model(model, pretrained_path, lora_path, strict=False):\n",
    "    model.load_state_dict(torch.load(pretrained_path), strict=strict)\n",
    "    model.load_state_dict(torch.load(lora_path), strict=strict)\n",
    "\n",
    "def print_model_layers(model, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints out the model's layers and their types.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        print(' ' * indent + f'{name}: {type(module).__name__}')\n",
    "        print_model_layers(module, indent + 2)\n",
    "\n",
    "def has_lora_layers(model):\n",
    "    \"\"\"\n",
    "    Check if the model has any LoRA layers.1\n",
    "    \"\"\"\n",
    "    for n, _ in model.named_parameters():\n",
    "        if 'lora_' in n:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LoRA layers found in the model.\n"
     ]
    }
   ],
   "source": [
    "if has_lora_layers(whisper_model):\n",
    "    print(\"LoRA layers have been successfully integrated into the model.\")\n",
    "else:\n",
    "    print(\"No LoRA layers found in the model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'r': 32, 'target_modules': ['query', 'key', 'value']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_attention_layers_with_lora(whisper_model, config=config)\n",
    "mark_only_lora_as_trainable(whisper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layers have been successfully integrated into the model.\n"
     ]
    }
   ],
   "source": [
    "if has_lora_layers(whisper_model):\n",
    "    print(\"LoRA layers have been successfully integrated into the model.\")\n",
    "else:\n",
    "    print(\"No LoRA layers found in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 38,069,376 parameters, 884,736 are trainable, a reduction of 97.7 % \n"
     ]
    }
   ],
   "source": [
    "def print_trainable_params(model: nn.Module) -> None:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Out of {total_params:,} parameters, {trainable_params:,} are trainable, a reduction of {round((1-trainable_params/total_params)*100, 1)} % \")\n",
    "print_trainable_params(whisper_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
